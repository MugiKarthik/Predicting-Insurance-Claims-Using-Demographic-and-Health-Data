
datapreprocess
# One-hot encoding for 'region'
data = pd.get_dummies(data, columns=['region','sex','smoker'])

#model#
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc,recall_score,precision_score

X = data.drop("insuranceclaim", axis=1)
y = data["insuranceclaim"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
# Train the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)
# Calculate the probabilities for each prediction
y_prob = model.predict_proba(X_test)[:, 1]


# Calculate the false positive rate, true positive rate, and thresholds for the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)

# Calculate the area under the ROC curve
roc_auc = auc(fpr, tpr)

# plot the ROC curve
plt.figure()
plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--', label="Random guess")
plt.xlabel("False positive rate")
plt.ylabel("True positive rate")
plt.title("Receiver operating characteristic (ROC) curve")
plt.legend()
plt.show()

# Calculate the sensitivity, specificity, positive predictive value, and negative predictive value
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
sensitivity = tp / (tp + fn)
specificity = tn / (tn + fp)
ppv = tp / (tp + fp)
npv = tn / (tn + fn)
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)
print("Positive Predictive Value:", ppv)
print("Negative Predictive Value:", npv)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
# Evaluate the performance of the model

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))









#model#
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc,recall_score,precision_score


import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.metrics import roc_curve, auc

# Fit the logistic regression model on the training data
model = sm.Logit(y_train, X_train)
result = model.fit()
result.summary()
# Make predictions on the test data
y_prob = result.predict(X_test)

# Calculate the false positive rate, true positive rate, and thresholds for the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)

# Find the optimal cut value that maximizes the Youden's index (i.e., sensitivity + specificity - 1)
youden_index = tpr - fpr
optimal_idx = np.argmax(youden_index)
optimal_threshold = thresholds[optimal_idx]

# Make binary predictions based on the cut value
y_pred = (y_prob >= optimal_threshold).astype(int)

# Print the confusion matrix and classification report
from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Calculate the area under the ROC curve
roc_auc = auc(fpr, tpr)


# plot the ROC curve
plt.figure()
plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--', label="Random guess")
plt.plot([0, fpr[optimal_idx]], [tpr[optimal_idx], tpr[optimal_idx]], 'r--', label="Optimal cut value")
plt.xlabel("False positive rate")
plt.ylabel("True positive rate")
plt.title("Receiver operating characteristic (ROC) curve")
plt.legend()
plt.show()

# Calculate the sensitivity, specificity, positive predictive value, and negative predictive value
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
sensitivity = tp / (tp + fn)
specificity = tn / (tn + fp)
ppv = tp / (tp + fp)
npv = tn / (tn + fn)
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)
print("Positive Predictive Value:", ppv)
print("Negative Predictive Value:", npv)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
# Evaluate the performance of the model
# plot the confusion matrix as a heatmap
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap="coolwarm", fmt="d")
plt.title("Confusion matrix")
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.show()

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))














from sklearn.tree import DecisionTreeClassifier, plot_tree
# train decision tree model
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# predict on test set and calculate performance metrics
y_pred2 = dt.predict(X_test)
y_prob2 = dt.predict_proba(X_test)[:, 1]

# plot decision tree diagram
fig, ax = plt.subplots(figsize=(12, 12))
plot_tree(dt, ax=ax, feature_names=X.columns, class_names=["0", "1"], filled=True)
plt.show()

# Calculate the false positive rate, true positive rate, and thresholds for the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob2)

# Calculate the area under the ROC curve
roc_auc2 = auc(fpr, tpr)

# plot the ROC curve
plt.figure()
plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc2)
plt.plot([0, 1], [0, 1], 'k--', label="Random guess")
plt.xlabel("False positive rate")
plt.ylabel("True positive rate")
plt.title("Receiver operating characteristic (ROC) curve")
plt.legend()
plt.show()

# Calculate the sensitivity, specificity, positive predictive value, and negative predictive value
tn, fp, fn, tp = confusion_matrix(y_test, y_pred2).ravel()
sensitivity = tp / (tp + fn)
specificity = tn / (tn + fp)
ppv = tp / (tp + fp)
npv = tn / (tn + fn)
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)
print("Positive Predictive Value:", ppv)
print("Negative Predictive Value:", npv)
print("Accuracy:", accuracy_score(y_test, y_pred2))
print("Precision:", precision_score(y_test, y_pred2))
print("Recall:", recall_score(y_test, y_pred2))
# Evaluate the performance of the model

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred2))
print("Classification Report:\n", classification_report(y_test, y_pred2))



from sklearn.ensemble import RandomForestClassifier
rf= RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred1=rf.predict(X_test)
y_prob1 = rf.predict_proba(X_test)[:, 1]

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(50, 15), dpi=300)
for i in range(3):
    plot_tree(rf.estimators_[i],feature_names=X_train.columns, impurity=False, proportion=True, filled=True, ax=axes[i])
    axes[i].set_title(f"Decision Tree {i+1}")
plt.show()
# Calculate the false positive rate, true positive rate, and thresholds for the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob1)

# Calculate the area under the ROC curve
roc_auc1 = auc(fpr, tpr)

# plot the ROC curve
plt.figure()
plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc1)
plt.plot([0, 1], [0, 1], 'k--', label="Random guess")
plt.xlabel("False positive rate")
plt.ylabel("True positive rate")
plt.title("Receiver operating characteristic (ROC) curve")
plt.legend()
plt.show()

# Calculate the sensitivity, specificity, positive predictive value, and negative predictive value
tn, fp, fn, tp = confusion_matrix(y_test, y_pred1).ravel()
sensitivity = tp / (tp + fn)
specificity = tn / (tn + fp)
ppv = tp / (tp + fp)
npv = tn / (tn + fn)
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)
print("Positive Predictive Value:", ppv)
print("Negative Predictive Value:", npv)
print("Accuracy:", accuracy_score(y_test, y_pred1))
print("Precision:", precision_score(y_test, y_pred1))
print("Recall:", recall_score(y_test, y_pred1))
# Evaluate the performance of the model

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred1))
print("Classification Report:\n", classification_report(y_test, y_pred1))
